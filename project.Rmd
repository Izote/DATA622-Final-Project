---
title: "DATA 622: Final Project"
author: "Group 3"
date: "12/10/2021"
output: html_document
---

# EDA

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, echo = FALSE, warning = FALSE, message = FALSE
  )

library(tidyverse)
library(e1071)

recipes <- read_csv("./recipes.csv", show_col_types = FALSE)
reviews <- read_csv("./reviews.csv", show_col_types = FALSE)
```

While strategic sampling is a general best practice, this specific data set is large and we will need to be intentional in our approach to selecting observations. While we would eventually settle on a particular set of predictors and target variable, there was little practical was knowledge of this data set beyond its context and general structure. We explored the data to get a sense of what would be feasibly modeled and gain and understanding of the limitations and scope of the data.

```{r}
get_ranks <- function(df, variable, order_levels, group_name = "rank") {
  df <- df %>% 
    mutate(across(variable, factor, levels = order_levels, ordered = TRUE)) %>%
    group_by(across(variable)) %>% 
    tally()
  
  if (variable != group_name) {
    colnames(df)[[1]] <- group_name
  }
  
  df$variable <- variable
  df
}

ratings <- seq(0, 5, 0.5)
review_ranks <- get_ranks(reviews, "Rating", ratings)
recipe_ranks <- get_ranks(recipes, "AggregatedRating", ratings)
```

An initial point of interest was aggregated ratings for the recipes. In visualizing ratings (global actual reviews across the site and aggregated figures for the available recipes), there were two primary revelations:

* Users seem to take an all-or-nothing approach to reviewing. That is, 5-star ratings hugely overwhelm other all ratings.
* Nearly half of our given recipe data set is missing an aggregate rating. 

```{r}
bind_rows(review_ranks, recipe_ranks) %>% 
  mutate(across(
    "variable", 
    ~ if_else(. == "Rating", "Global Ratings (Actual)", "Available Subset (Aggregated)")
    )) %>% 
  ggplot(aes(rank, n)) +
  geom_col() +
  scale_y_continuous(labels = scales::comma_format()) +
  facet_wrap("variable", nrow = 2, scales = "free") +
  labs(title = "Food.com Recipe Ratings", x = "Rating", y = "Count")
```

While having such a large proportion of missing ratings might be alarming, dropping those observations from use would still net a sizable data set to work with. More troubling is that the ratings themselves appear to follow an "all or nothing" approach. That is, users tend only to review what they feel very positively about. Downstream analysis lead to the eventual abandonment of reviews/ratings as an area of interest due to its seeming lack of relationship with rich variety of nutritional variables present in the data coupled with the reality that a binary outcome on recipe success was not particularly interesting. 

On the other hand nutritional variables provide quantifiable, numeric measurements for each recipe. Many of the non-nutritional variables are identifying elements utilized by the website the data were sourced from. With the decision to move forward and build out a framework to predict the fiber content of a recipe based on its other nutritional dimensions, we can learn about this target variable by visualizing it as we've done below.

```{r}
get_boundaries <- function(x) {median(x) + mad(x) * c(-3, 3)}

recipe_dbl <- recipes %>%
  select(
    where(is_double),
    -AuthorId, -AggregatedRating, -DatePublished, -ReviewCount, -RecipeServings
    ) %>%
  drop_na()

boundary <- recipe_dbl %>%
  select(-RecipeId) %>%
  as.list() %>%
  map(get_boundaries)

recipe_dbl <- recipe_dbl %>%
  filter(
      between(Calories, boundary$Calories[[1]], boundary$Calories[[2]]) &
      between(FatContent, boundary$FatContent[[1]], boundary$FatContent[[2]]) &
      between(SaturatedFatContent, boundary$SaturatedFatContent[[1]], boundary$SaturatedFatContent[[2]]) &
      between(CholesterolContent, boundary$CholesterolContent[[1]], boundary$CholesterolContent[[2]]) &
      between(SodiumContent, boundary$SodiumContent[[1]], boundary$SodiumContent[[2]]) &
      between(CarbohydrateContent, boundary$CarbohydrateContent[[1]], boundary$CarbohydrateContent[[2]]) &
      between(FiberContent, boundary$FiberContent[[1]], boundary$FiberContent[[2]]) &
      between(SugarContent, boundary$SugarContent[[1]], boundary$SugarContent[[2]]) &
      between(ProteinContent, boundary$ProteinContent[[1]], boundary$ProteinContent[[2]])
      )
```

```{r}
recipe_dbl %>% 
  ggplot(aes(FiberContent)) +
  geom_histogram(bins = 10) +
  scale_x_continuous(labels = scales::comma_format(1)) +
  scale_y_continuous(labels = scales::comma_format()) +
  labs(
    title = "Food.com Recipes by Fiber Content",
    x = "Fiber (g)", 
    y = "Number of Recipes"
    )
```

There is insufficient context and domain knowledge for us to impute missing values for any of the nutritional variables we wished to work with, and we had a very large data set to begin with, so rows with missing values were dropped from use. We also opted to ignore publication date, review counts and recipe servings which should all have no impact on the nutritional value of a given recipe. In particular, recipe servings were deemed subjective as these counts rely on too many elements external to the data. Lastly, outliers were detected and removed by evaluating rows with values outside a range of three mean absolute deviations from the median for each nutritional variable. 

Knowing we wished to utilize the nurtitional variables, we assessed the correlation between these variables. Specifying fiber content as our dependent or target variable, we can see that there is a moderate relationship between it and both the caloric and carbohydrate content of a recipe. Fat, sodium and protein content also showed a set of weak relationships with fiber content. It is notable that multicollinearity was present across the various nutritional variables, which makes intuitive sense.

```{r}
cor(select(recipe_dbl, -RecipeId))
```

Given the interrelated nature of the nutritional variables, they are good candidates for principal component analysis (PCA). This will provide us the benefit of not only side-stepping multicollinearity issues by creating orthogonal variables, but we will also simplify our downstream modeling process as we will work with fewer variables. We will lose interpretability, as the PCs themselves will be difficult to express linguistically, but this is not something we're concerned about in this scenario. We prepare the data by scaling and centering the variables and produce the results below.

```{r}
recipe_pca <- recipe_dbl %>%
  select(-c("RecipeId", "FiberContent")) %>% 
  prcomp(scale = TRUE, center = TRUE)

summary(recipe_pca)
```

Based on the summary output above, we can see that half of the variability can be explained within the first principal component (PC). Including more of the initial PCs, we can up this proportion and achieve just under 95% with the first five PCs. Regardless of our approach downstream, utilizing up to a handful of these variables means we've reduced our number of predictors by at least half.

# Modeling

Using the `e1071` library, a function was prepared to optimize the cost parameter of potential SVM models, along with a pair of wrapper functions to generate evaluation measures. Our approach was to parse through a series of potential cost values and ultimately selecting the best performing model, implementing 5-fold cross validation to help improve performance. In order to keep the computational overhead of the SVM approach minimal, we limited ourselves to using an overall sample of 1,000 observations - with 80% reserved for training the model and 20% for our final test. The code included with this report represents prior experimentation with kernel selection, which lead to the radial basis function kernel being the best performing in this circumstance. 

# SVM

```{r}
try_svm <- function(
  data, type, kernel, k = 5
  ) {
  tune.svm(
    FiberContent ~ PC1 + PC2 + PC3 + PC4 + PC5,
    data = data,
    type = type,
    cost = seq(0.01, 1, 0.01),
    tolerance = seq(0.001, 0.01, 0.001),
    gamma = seq(0.1, 0.2, 0.01),
    kernel = kernel,
    cross = k
    )
  }

eval_regr <- function(data, model, new = FALSE) {
  if (new) {
    prediction <- predict(model, newdata = data)
  } else {
    prediction <- predict(model)
  }
  
  list(
    MSE = Metrics::mse(data$FiberContent, prediction),
    RMSE = Metrics::rmse(data$FiberContent, prediction),
    MAE = Metrics::mae(data$FiberContent, prediction)
  )
}
```


```{r}
set.seed(3)

full_pca_data <- bind_cols(
  select(recipe_dbl, RecipeId, FiberContent),
  as_tibble(recipe_pca$x)
  )

train <- slice_sample(full_pca_data, n = 800)
test <- slice_sample(anti_join(full_pca_data, train, by = "RecipeId"), n = 200)

# svm_regr <- try_svm(train, type = "eps-regression", kernel = "radial")
# best_regr <- svm_regr$best.model

svm_regr <- svm(
  FiberContent ~ PC1 + PC2 + PC3 + PC4 + PC5,
  type = "eps-regression",
  kernel = "radial",
  data = train,
  cost = 1,
  gamma = 0.12,
  epsilon = 0.2,
)
```


```{r}
(regr_eval <- eval_regr(train, svm_regr))
```
```{r}
(regr_eval2 <- eval_regr(test, svm_regr, new = TRUE))
```